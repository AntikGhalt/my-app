# ISTAT/EUROSTAT Data Pipeline

Automated data pipeline that fetches statistical data from ISTAT/EUROSTAT APIs and uploads processed Excel files to Google Drive with intelligent version management.

## Overview

This system runs on **Google Cloud Run** and is triggered by **Cloud Scheduler**. It downloads data from statistical APIs, processes it into Excel files, and uploads them to a Google Shared Drive.

### Key Features

- **Auto-discovery**: Just add a new pipeline file in `pipelines/` folder - no code changes needed
- **Smart versioning**: Supports both Edition-based and DateDownload-based archiving
- **Automatic archiving**: Old files are moved to Archive folder with version suffix
- **Fixed filenames**: Output files always have `_LATEST.xlsx` suffix for Tableau compatibility
- **Logging**: All operations are logged to `pipeline_log.txt` in Google Drive

---

## Project Structure

```
my-app/
├── Dockerfile              # Container configuration
├── main.py                 # Flask app with auto-discovery and smart upload
├── requirements.txt        # Python dependencies
├── README.md               # This file
└── pipelines/
    ├── __init__.py                         # Package marker
    ├── istat_reddito_famiglie.py           # ISTAT household income (Edition-based)
    ├── istat_reddito_famiglie_test.py      # Test pipeline
    └── example_datedownload_pipeline.py    # Template for DateDownload pipelines
```

---

## Versioning System

The pipeline supports two versioning modes:

### 1. Edition-based Versioning

Used for data sources that have official edition/release identifiers (e.g., ISTAT releases).

**How it works:**
- Pipeline returns `edition` value (e.g., "2025M10")
- File is archived only when edition changes
- Archive name: `FILE_2025M10_Edition.xlsx`

**In pipeline code:**
```python
return {
    'status': 'success',
    'buffer': buffer,
    'filename': 'MyData_LATEST.xlsx',
    'edition': '2025M10',  # <-- Triggers Edition-based versioning
    ...
}
```

**In Excel Metadati sheet:**
```
chiave          valore
edition         2025M10
edition_type    Edition
download_date   2025-12-04 12:30:00
```

### 2. DateDownload-based Versioning

Used for data sources without official editions. Archives monthly based on download date.

**How it works:**
- Pipeline returns `edition: None` or empty
- File is archived when month changes
- Archive name: `FILE_2025M12_DateDownload.xlsx`

**In pipeline code:**
```python
return {
    'status': 'success',
    'buffer': buffer,
    'filename': 'MyData_LATEST.xlsx',
    'edition': None,  # <-- Triggers DateDownload-based versioning
    ...
}
```

**In Excel Metadati sheet:**
```
chiave          valore
edition         
edition_type    DateDownload
download_date   2025-12-04 12:30:00
```

### Archive Naming Convention

| Versioning Type | Archive Filename |
|-----------------|------------------|
| Edition-based | `FILE_2025M10_Edition.xlsx` |
| DateDownload-based | `FILE_2025M12_DateDownload.xlsx` |
| Error/Missing metadata | `FILE_20251204_123000_ErrorNoMetadata.xlsx` |

---

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Health check |
| `/test` | GET | Test Google Drive connection |
| `/pipelines` | GET | List all available pipelines |
| `/run` | GET/POST | Run default pipeline (backward compatible) |
| `/run/<pipeline_name>` | GET/POST | Run specific pipeline by name |
| `/run/all` | GET/POST | Run all pipelines sequentially |

### Examples

```bash
# Health check
curl https://your-service.run.app/

# List available pipelines
curl https://your-service.run.app/pipelines

# Run specific pipeline
curl https://your-service.run.app/run/istat_reddito_famiglie

# Run all pipelines
curl https://your-service.run.app/run/all
```

---

## Adding a New Pipeline

### Step 1: Create pipeline file

Create `pipelines/your_pipeline_name.py`:

```python
"""
Your Pipeline Name
==================
Description of what this pipeline does.

Versioning: Edition-based OR DateDownload-based
"""

import io
import pandas as pd
from datetime import datetime
from openpyxl.utils import get_column_letter

OUTPUT_FILENAME = "Your_data_LATEST.xlsx"

def run_pipeline() -> dict:
    """
    Execute the pipeline.
    
    Must return dict with:
    - status: 'success' or 'error'
    - buffer: BytesIO with Excel file (if success)
    - filename: OUTPUT_FILENAME
    - edition: Edition string OR None for DateDownload
    - message: Error message (if error)
    
    Optional metadata:
    - n_variables, n_observations, period_range, etc.
    """
    try:
        # 1. Download data
        # 2. Process data
        # 3. Create Excel with proper Metadati sheet
        
        download_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
            # Metadati - REQUIRED for versioning!
            meta = pd.DataFrame([
                ("edition", "2025M10"),        # Or empty for DateDownload
                ("edition_type", "Edition"),   # Or "DateDownload"
                ("download_date", download_date),
                # ... other metadata
            ], columns=["chiave", "valore"])
            meta.to_excel(writer, sheet_name="Metadati", index=False)
            
            # Your data
            df.to_excel(writer, sheet_name="Dati", index=False)
        
        return {
            'status': 'success',
            'buffer': buffer,
            'filename': OUTPUT_FILENAME,
            'edition': '2025M10',  # Or None for DateDownload
            'n_variables': 10,
            'n_observations': 100,
        }
        
    except Exception as e:
        return {'status': 'error', 'message': str(e)}
```

### Step 2: Deploy

```bash
gcloud run deploy istat-pipeline --source . --region europe-west1 --timeout=300 --memory=1Gi
```

That's it! The pipeline is automatically discovered and available at `/run/your_pipeline_name`.

---

## Required Excel Metadati Fields

For proper versioning, include these fields in the Metadati sheet:

| Field | Required | Description |
|-------|----------|-------------|
| `edition` | Yes* | Edition identifier (e.g., "2025M10"). Empty for DateDownload. |
| `edition_type` | Yes | Either "Edition" or "DateDownload" |
| `download_date` | Yes | Timestamp of data download |

*Can be empty if edition_type is "DateDownload"

---

## Configuration

### Google Drive IDs

Set via environment variables or defaults in `main.py`:

```python
DRIVE_FOLDER_ID = "0ACZ58HkBSJpjUk9PVA"           # Main folder (DATABASE3)
ARCHIVE_FOLDER_ID = "1wT0j1Hz26TW9v891LQ2ZFSpGHwQkkAmu"  # Archive subfolder
```

### Service Account

The pipeline uses the default compute service account which must have **Content Manager** access to:
- Main Shared Drive folder
- Archive subfolder

---

## Deployment Commands

### Deploy or update

```bash

gcloud run deploy istat-pipeline \
  --source . \
  --region europe-west1 \
  --allow-unauthenticated \
  --timeout=300 \
  --memory=1Gi

```

### Cloud Scheduler

```bash
# Create daily job at 8:30 AM Rome time (runs all pipelines)
gcloud scheduler jobs create http istat-daily-update \
  --location=europe-west1 \
  --schedule="30 8 * * *" \
  --uri="https://istat-pipeline-953005439278.europe-west1.run.app/run/all" \
  --http-method=GET \
  --oidc-service-account-email=953005439278-compute@developer.gserviceaccount.com \
  --time-zone="Europe/Rome"

# Test manually
gcloud scheduler jobs run istat-daily-update --location=europe-west1

# Update existing job
gcloud scheduler jobs update http istat-daily-update \
  --location=europe-west1 \
  --uri="https://istat-pipeline-953005439278.europe-west1.run.app/run/all"
```

**Note:** Google Cloud free tier includes 3 scheduler jobs. Using `/run/all` with one scheduler is recommended.

---

## Troubleshooting

### Check logs

```bash
gcloud run services logs read istat-pipeline --region=europe-west1 --limit=50
```

### Test endpoints

```bash

SERVICE_URL="https://istat-pipeline-953005439278.europe-west1.run.app"
echo "=== Health ===" && curl "$SERVICE_URL/"
echo -e "\n=== Drive ===" && curl "$SERVICE_URL/test"
echo -e "\n=== Pipelines ===" && curl "$SERVICE_URL/pipelines"
echo -e "\n=== Run All ===" && curl "$SERVICE_URL/run/all"


```bash

SERVICE_URL="https://istat-pipeline-953005439278.europe-west1.run.app"
#chose whichone

# Reddito famiglie (principale)
curl "$SERVICE_URL/run/istat_reddito_famiglie"

# Reddito famiglie test (senza rettifica)
curl "$SERVICE_URL/run/istat_reddito_famiglie_test"

# Example datedownload (se lo aggiungi)
curl "$SERVICE_URL/run/example_datedownload_pipeline"

```

### Common errors

| Error | Cause | Solution |
|-------|-------|----------|
| 404 Not Found | Pipeline not found | Check filename matches endpoint name |
| 403 Drive error | Permission issue | Verify service account has Content Manager access |
| No edition found | API changed | Check ISTAT API availability |
| Timeout | Slow download | Increase `--timeout` in deploy command |

---

## File Locations

### Main folder (DATABASE3)
```
Reddito_disponibile_famiglie_LATEST.xlsx    ← Tableau connects here
pipeline_log.txt                             ← Operation log
```

### Archive folder
```
Reddito_disponibile_famiglie_2025M10_Edition.xlsx
Reddito_disponibile_famiglie_2025M09_Edition.xlsx
OtherData_2025M11_DateDownload.xlsx
```

---

## Log Format

```
[2025-12-04 12:30:00] Pipeline_name: updated; version: 2025M10_Edition
[2025-12-04 12:30:00] Pipeline_name: not_updated; version: 2025M10_Edition (unchanged)
[2025-12-04 12:30:00] Pipeline_name: error; Error message here
```

---

## Contact

Project maintained by Paolo Refuto.

For issues or modifications, share this README with Claude to provide context.