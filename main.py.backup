# main.py
"""
ISTAT/EUROSTAT Data Pipeline - Main Application
================================================
Flask application with AUTO-DISCOVERY of pipelines.
Just add a new file in pipelines/ folder and it will be available automatically.

Author: Paolo Refuto
Last Updated: December 2025
"""
import os
import io
import importlib
import pkgutil
from datetime import datetime
from flask import Flask, jsonify
from google.auth import default
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload

# =============================================================================
# FLASK APP INITIALIZATION
# =============================================================================
app = Flask(__name__)

# =============================================================================
# CONFIGURATION
# =============================================================================
DRIVE_FOLDER_ID = os.environ.get("DRIVE_FOLDER_ID", "0ACZ58HkBSJpjUk9PVA")
ARCHIVE_FOLDER_ID = os.environ.get("ARCHIVE_FOLDER_ID", "1wT0j1Hz26TW9v891LQ2ZFSpGHwQkkAmu")
LOG_FILENAME = "pipeline_log.txt"

# =============================================================================
# GOOGLE DRIVE UTILITIES
# =============================================================================

def get_drive_service():
    credentials, project = default(scopes=['https://www.googleapis.com/auth/drive'])
    return build('drive', 'v3', credentials=credentials)

def find_file_by_name(filename: str, folder_id: str) -> dict | None:
    service = get_drive_service()
    query = f"name = '{filename}' and '{folder_id}' in parents and trashed = false"
    results = service.files().list(
        q=query, spaces='drive', fields='files(id, name)',
        supportsAllDrives=True, includeItemsFromAllDrives=True,
        corpora='drive', driveId=DRIVE_FOLDER_ID
    ).execute()
    files = results.get('files', [])
    return files[0] if files else None

def get_edition_from_excel(file_id: str) -> str | None:
    import pandas as pd
    service = get_drive_service()
    try:
        request = service.files().get_media(fileId=file_id, supportsAllDrives=True)
        buffer = io.BytesIO()
        downloader = MediaIoBaseDownload(buffer, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
        buffer.seek(0)
        df_meta = pd.read_excel(buffer, sheet_name='Metadati', nrows=10)
        for idx, row in df_meta.iterrows():
            if row.iloc[0] == 'edition':
                return str(row.iloc[1])
        return None
    except Exception as e:
        print(f"Error reading edition: {e}")
        return None

def move_file_to_archive(file_id: str, filename: str, edition: str, archive_folder_id: str) -> bool:
    service = get_drive_service()
    try:
        archived_name = filename.replace('_LATEST', f'_{edition}')
        file = service.files().get(fileId=file_id, fields='parents', supportsAllDrives=True).execute()
        previous_parents = ",".join(file.get('parents', []))
        service.files().update(
            fileId=file_id, addParents=archive_folder_id, removeParents=previous_parents,
            body={'name': archived_name}, fields='id, parents', supportsAllDrives=True
        ).execute()
        return True
    except Exception as e:
        print(f"Error archiving: {e}")
        return False

def upload_excel_to_drive(buffer: io.BytesIO, filename: str, folder_id: str) -> tuple[str, str]:
    service = get_drive_service()
    buffer.seek(0)
    file_metadata = {'name': filename, 'parents': [folder_id], 'driveId': folder_id, 'supportsAllDrives': True}
    media = MediaIoBaseUpload(buffer, mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
    file = service.files().create(body=file_metadata, media_body=media, fields='id, webViewLink', supportsAllDrives=True).execute()
    return file.get('id'), file.get('webViewLink')

def smart_upload(buffer: io.BytesIO, filename: str, edition: str, folder_id: str, archive_folder_id: str) -> dict:
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    existing_file = find_file_by_name(filename, folder_id)
    if existing_file:
        existing_edition = get_edition_from_excel(existing_file['id'])
        if existing_edition == edition:
            return {'status': 'not_updated', 'reason': 'Edition unchanged', 'edition': edition, 'filename': filename, 'timestamp': timestamp}
        else:
            move_file_to_archive(existing_file['id'], filename, existing_edition or 'unknown', archive_folder_id)
    file_id, web_link = upload_excel_to_drive(buffer, filename, folder_id)
    return {'status': 'updated', 'edition': edition, 'filename': filename, 'file_id': file_id, 'web_link': web_link, 'timestamp': timestamp}

def update_log(pipeline_name: str, status: str, edition: str, details: str = "") -> bool:
    service = get_drive_service()
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    if status == "updated":
        log_line = f"[{timestamp}] {pipeline_name}: updated; edition: {edition}"
    elif status == "not_updated":
        log_line = f"[{timestamp}] {pipeline_name}: not_updated; edition: {edition} (unchanged)"
    else:
        log_line = f"[{timestamp}] {pipeline_name}: error; {details}"
    try:
        existing_log = find_file_by_name(LOG_FILENAME, DRIVE_FOLDER_ID)
        if existing_log:
            request = service.files().get_media(fileId=existing_log['id'], supportsAllDrives=True)
            buffer = io.BytesIO()
            downloader = MediaIoBaseDownload(buffer, request)
            done = False
            while not done:
                _, done = downloader.next_chunk()
            buffer.seek(0)
            existing_content = buffer.read().decode('utf-8')
            new_content = existing_content + log_line + "\n"
            media = MediaIoBaseUpload(io.BytesIO(new_content.encode('utf-8')), mimetype='text/plain')
            service.files().update(fileId=existing_log['id'], media_body=media, supportsAllDrives=True).execute()
        else:
            file_metadata = {'name': LOG_FILENAME, 'parents': [DRIVE_FOLDER_ID], 'mimeType': 'text/plain'}
            media = MediaIoBaseUpload(io.BytesIO((log_line + "\n").encode('utf-8')), mimetype='text/plain')
            service.files().create(body=file_metadata, media_body=media, fields='id', supportsAllDrives=True).execute()
        return True
    except Exception as e:
        print(f"Error updating log: {e}")
        return False

# =============================================================================
# AUTO-DISCOVERY OF PIPELINES
# =============================================================================

def discover_pipelines() -> dict:
    """
    Automatically discover all pipeline modules in the pipelines/ folder.
    Each module must have a run_pipeline() function.
    Returns dict: {pipeline_name: module}
    """
    pipelines = {}
    try:
        import pipelines as pipelines_pkg
        for importer, modname, ispkg in pkgutil.iter_modules(pipelines_pkg.__path__):
            if not ispkg and not modname.startswith('_'):
                try:
                    module = importlib.import_module(f'pipelines.{modname}')
                    if hasattr(module, 'run_pipeline'):
                        pipelines[modname] = module
                        print(f"[Discovery] Found pipeline: {modname}")
                except Exception as e:
                    print(f"[Discovery] Error loading {modname}: {e}")
    except Exception as e:
        print(f"[Discovery] Error scanning pipelines folder: {e}")
    return pipelines

def run_single_pipeline(pipeline_name: str, module) -> dict:
    """Execute a single pipeline and handle upload."""
    try:
        result = module.run_pipeline()
        if result['status'] == 'error':
            update_log(pipeline_name, "error", "", result.get('message', ''))
            return result
        upload_result = smart_upload(
            buffer=result['buffer'],
            filename=result['filename'],
            edition=result['edition'],
            folder_id=DRIVE_FOLDER_ID,
            archive_folder_id=ARCHIVE_FOLDER_ID
        )
        update_log(pipeline_name, upload_result['status'], result['edition'])
        upload_result['n_variables'] = result.get('n_variables')
        upload_result['n_observations'] = result.get('n_observations')
        upload_result['period_range'] = result.get('period_range')
        upload_result['sector'] = result.get('sector')
        return upload_result
    except Exception as e:
        import traceback
        traceback.print_exc()
        update_log(pipeline_name, "error", "", str(e))
        return {'status': 'error', 'message': str(e)}

# =============================================================================
# FLASK ENDPOINTS
# =============================================================================

@app.route('/')
def health_check():
    return jsonify({'status': 'healthy', 'service': 'istat-pipeline', 'timestamp': datetime.now().isoformat()})

@app.route('/test')
def test_drive():
    try:
        service = get_drive_service()
        results = service.files().list(
            q=f"'{DRIVE_FOLDER_ID}' in parents", pageSize=10, fields="files(id, name)",
            supportsAllDrives=True, includeItemsFromAllDrives=True, corpora='drive', driveId=DRIVE_FOLDER_ID
        ).execute()
        files = results.get('files', [])
        return jsonify({
            'status': 'success', 'message': 'Drive connection working',
            'folder_id': DRIVE_FOLDER_ID, 'archive_folder_id': ARCHIVE_FOLDER_ID,
            'files_in_folder': len(files), 'sample_files': [f['name'] for f in files],
            'timestamp': datetime.now().isoformat()
        }), 200
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e), 'timestamp': datetime.now().isoformat()}), 500

@app.route('/pipelines')
def list_pipelines():
    """List all available pipelines."""
    pipelines = discover_pipelines()
    return jsonify({
        'status': 'success',
        'available_pipelines': list(pipelines.keys()),
        'endpoints': [f'/run/{name}' for name in pipelines.keys()],
        'timestamp': datetime.now().isoformat()
    })

@app.route('/run/<pipeline_name>', methods=['GET', 'POST'])
def run_pipeline_by_name(pipeline_name: str):
    """
    Run any pipeline by name.
    Example: /run/istat_reddito_famiglie
    """
    pipelines = discover_pipelines()
    
    if pipeline_name not in pipelines:
        return jsonify({
            'status': 'error',
            'message': f'Pipeline "{pipeline_name}" not found',
            'available_pipelines': list(pipelines.keys())
        }), 404
    
    result = run_single_pipeline(pipeline_name, pipelines[pipeline_name])
    status_code = 200 if result.get('status') != 'error' else 500
    return jsonify(result), status_code

@app.route('/run', methods=['GET', 'POST'])
def run_default():
    """Run the default pipeline (istat_reddito_famiglie) for backward compatibility."""
    return run_pipeline_by_name('istat_reddito_famiglie')

@app.route('/run/all', methods=['GET', 'POST'])
def run_all():
    """Run all discovered pipelines sequentially."""
    pipelines = discover_pipelines()
    results = {}
    
    for name, module in pipelines.items():
        print(f"[run/all] Running pipeline: {name}")
        results[name] = run_single_pipeline(name, module)
    
    return jsonify({
        'status': 'completed',
        'pipelines_run': len(results),
        'results': results,
        'timestamp': datetime.now().isoformat()
    })

@app.errorhandler(404)
def handle_not_found(error):
    pipelines = discover_pipelines()
    return jsonify({
        'status': 'error', 'message': 'Route not found',
        'available_routes': ['/', '/test', '/pipelines', '/run', '/run/all'] + [f'/run/{p}' for p in pipelines.keys()],
        'timestamp': datetime.now().isoformat()
    }), 404

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port, debug=True)
